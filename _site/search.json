[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Karl Forner’s blog",
    "section": "",
    "text": "Preparing Rfuzzycoco for publication on CRAN\n\n\n\n\n\n\nR\n\n\nfuzzycoco\n\n\nc++\n\n\n\n\n\n\n\n\n\nOct 1, 2025\n\n\nKarl Forner\n\n\n\n\n\n\n\n\n\n\n\n\nfuzzycoco: C++ open-source release of my re-implementation of the Fuzzy Coco algorithm\n\n\n\n\n\n\nfuzzycoco\n\n\nc++\n\n\n\n\n\n\n\n\n\nSep 10, 2025\n\n\nKarl Forner\n\n\n\n\n\n\n\n\n\n\n\n\nOrganizing R development using srcpkgs\n\n\n\n\n\n\nR\n\n\nsrcpkgs\n\n\ndev\n\n\n\n\n\n\n\n\n\nJul 18, 2025\n\n\nKarl Forner\n\n\n\n\n\n\n\n\n\n\n\n\nan elegant way to fix user IDs in docker containers using docker_userid_fixer\n\n\n\n\n\n\ndocker\n\n\nreproducible_research\n\n\ndevops\n\n\n\n\n\n\n\n\n\nAug 14, 2024\n\n\nKarl Forner\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "contributions.html",
    "href": "contributions.html",
    "title": "my OpenSource contributions",
    "section": "",
    "text": "Perl\n\nDBIx::Class::LookupColumn (with Thomas Rubbatel) - https://metacpan.org/pod/DBIx::Class::LookupColumn::Auto\n\nR\n\nsrcpkgs: https://github.com/kforner/srcpkgs\nRcppProgress: https://github.com/kforner/rcpp_progress\nPlinker: https://github.com/kforner/plinker_pkg (note: not on CRAN)\nquartzbio.edp: https://github.com/kforner/quartzbio.edp\nallelic (fueatest): https://cran.r-project.org/web/packages/allelic/index.html (removed from CRAN), https://github.com/kforner/fueatest\nsnplinkage (with others): https://cran.r-project.org/web/packages/snplinkage/\n\n\n\n\n\nTransmart docker: https://github.com/kforner/docker_transmart\nR coverage patch: adding test coverage support in the R interpreter\n\nhttps://github.com/kforner/r-coverage-patch\nhttps://github.com/kforner/r-coverage-docker\n\n\n\n\n\n\n\n\n\ndebugme: https://github.com/r-lib/debugme/pull/21\ndevtools:\n\nhttps://github.com/r-lib/devtools/pull/334\n\n\ncovr:\n\nhttps://github.com/r-lib/covr/pull/202\nhttps://github.com/r-lib/covr/pull/193\nhttps://github.com/r-lib/covr/pull/123\nhttps://github.com/r-lib/covr/pull/32\nhttps://github.com/r-lib/covr/pull/30\n\nhttptest:\n\nhttps://github.com/nealrichardson/httptest/pull/79\nhttps://github.com/nealrichardson/httptest/pull/77\n\nhttptest2 - https://github.com/nealrichardson/httptest2/pull/22\nknitr:\n\nhttps://github.com/yihui/knitr/pull/1345\nhttps://github.com/yihui/knitr/pull/832\nhttps://github.com/yihui/knitr/pull/831\n\ntestthat:\n\nhttps://github.com/r-lib/testthat/pull/823\nhttps://github.com/r-lib/testthat/pull/268\nhttps://github.com/r-lib/testthat/pull/214\nhttps://github.com/r-lib/testthat/pull/120\nhttps://github.com/r-lib/testthat/pull/106\nhttps://github.com/r-lib/testthat/pull/98\nhttps://github.com/r-lib/testthat/pull/96\nhttps://github.com/r-lib/testthat/pull/85\n\n\n\n\n\n\ntrivy-offline - https://github.com/sighupio/trivy-offline/pull/13\nsast-parser - https://github.com/pcfens/sast-parser/pull/5\nmt-aws-glacier: https://github.com/vsespb/mt-aws-glacier/pull/24\nrocker:\n\nhttps://github.com/rocker-org/website/pull/4\nhttps://github.com/rocker-org/rocker-versioned/pull/57\nhttps://github.com/rocker-org/rocker-versioned/pull/7\nhttps://github.com/rocker-org/rocker-versioned/pull/5\nhttps://github.com/rocker-org/rocker-versioned/pull/3\n\nswig: Incorporated some patches to the Perl5 module related to           the -hide option and the destruction of objects.           Patch submitted by Karl Forner.\n\n\n\n\n\nrclone - https://github.com/rclone/rclone/pull/5556\n\n\n\n\n\n\nR\n\nR 3.0.3 - namespaceImportFrom() needed to detect primitive functions when checking for duplicated imports (reported by Karl Forner).\nR 3.0.2 - package.skeleton() was not starting its search for function objects in the correct place if environment was supplied. (Reported by Karl Forner.)\nR 3.0.1 - The return value when all workers failed from parallel::mclapply(mc.preschedule = TRUE) was a list of strings and not of error objects. (Spotted by Karl Forner and Bernd Bischl.)"
  },
  {
    "objectID": "contributions.html#published-opensource-projects",
    "href": "contributions.html#published-opensource-projects",
    "title": "my OpenSource contributions",
    "section": "",
    "text": "Perl\n\nDBIx::Class::LookupColumn (with Thomas Rubbatel) - https://metacpan.org/pod/DBIx::Class::LookupColumn::Auto\n\nR\n\nsrcpkgs: https://github.com/kforner/srcpkgs\nRcppProgress: https://github.com/kforner/rcpp_progress\nPlinker: https://github.com/kforner/plinker_pkg (note: not on CRAN)\nquartzbio.edp: https://github.com/kforner/quartzbio.edp\nallelic (fueatest): https://cran.r-project.org/web/packages/allelic/index.html (removed from CRAN), https://github.com/kforner/fueatest\nsnplinkage (with others): https://cran.r-project.org/web/packages/snplinkage/\n\n\n\n\n\nTransmart docker: https://github.com/kforner/docker_transmart\nR coverage patch: adding test coverage support in the R interpreter\n\nhttps://github.com/kforner/r-coverage-patch\nhttps://github.com/kforner/r-coverage-docker"
  },
  {
    "objectID": "contributions.html#contributions-to-existing-projects",
    "href": "contributions.html#contributions-to-existing-projects",
    "title": "my OpenSource contributions",
    "section": "",
    "text": "debugme: https://github.com/r-lib/debugme/pull/21\ndevtools:\n\nhttps://github.com/r-lib/devtools/pull/334\n\n\ncovr:\n\nhttps://github.com/r-lib/covr/pull/202\nhttps://github.com/r-lib/covr/pull/193\nhttps://github.com/r-lib/covr/pull/123\nhttps://github.com/r-lib/covr/pull/32\nhttps://github.com/r-lib/covr/pull/30\n\nhttptest:\n\nhttps://github.com/nealrichardson/httptest/pull/79\nhttps://github.com/nealrichardson/httptest/pull/77\n\nhttptest2 - https://github.com/nealrichardson/httptest2/pull/22\nknitr:\n\nhttps://github.com/yihui/knitr/pull/1345\nhttps://github.com/yihui/knitr/pull/832\nhttps://github.com/yihui/knitr/pull/831\n\ntestthat:\n\nhttps://github.com/r-lib/testthat/pull/823\nhttps://github.com/r-lib/testthat/pull/268\nhttps://github.com/r-lib/testthat/pull/214\nhttps://github.com/r-lib/testthat/pull/120\nhttps://github.com/r-lib/testthat/pull/106\nhttps://github.com/r-lib/testthat/pull/98\nhttps://github.com/r-lib/testthat/pull/96\nhttps://github.com/r-lib/testthat/pull/85\n\n\n\n\n\n\ntrivy-offline - https://github.com/sighupio/trivy-offline/pull/13\nsast-parser - https://github.com/pcfens/sast-parser/pull/5\nmt-aws-glacier: https://github.com/vsespb/mt-aws-glacier/pull/24\nrocker:\n\nhttps://github.com/rocker-org/website/pull/4\nhttps://github.com/rocker-org/rocker-versioned/pull/57\nhttps://github.com/rocker-org/rocker-versioned/pull/7\nhttps://github.com/rocker-org/rocker-versioned/pull/5\nhttps://github.com/rocker-org/rocker-versioned/pull/3\n\nswig: Incorporated some patches to the Perl5 module related to           the -hide option and the destruction of objects.           Patch submitted by Karl Forner.\n\n\n\n\n\nrclone - https://github.com/rclone/rclone/pull/5556"
  },
  {
    "objectID": "contributions.html#important-bug-reports-leading-to-official-fixes",
    "href": "contributions.html#important-bug-reports-leading-to-official-fixes",
    "title": "my OpenSource contributions",
    "section": "",
    "text": "R\n\nR 3.0.3 - namespaceImportFrom() needed to detect primitive functions when checking for duplicated imports (reported by Karl Forner).\nR 3.0.2 - package.skeleton() was not starting its search for function objects in the correct place if environment was supplied. (Reported by Karl Forner.)\nR 3.0.1 - The return value when all workers failed from parallel::mclapply(mc.preschedule = TRUE) was a list of strings and not of error objects. (Spotted by Karl Forner and Bernd Bischl.)"
  },
  {
    "objectID": "posts/fuzzycoco_release/index.html",
    "href": "posts/fuzzycoco_release/index.html",
    "title": "fuzzycoco: C++ open-source release of my re-implementation of the Fuzzy Coco algorithm",
    "section": "",
    "text": "I am pleased to announce the open-source (GPL-3) release of my re-implementation of the Fuzzy Coco algorithm: https://github.com/Lonza-RND-Data-Science/fuzzycoco.\nIn short, Fuzzy CoCo combines fuzzy logic with cooperative genetic algorithms to evolve clear, human-understandable models for explainable machine learning, cf Fuzzy CoCo: a cooperative-coevolutionary approach to fuzzy modeling from Carlos Andrés Peña-Reyes.\nThis is my re-implementation of the FUGE_LC C++ software, developed by Jean-Philippe Meylan, Yvan Da Silva and Rochus Keller (cf full acknowledgements).\nThe motivations for that re-implementation were mainly to be able to easily use and distribute this software using high-level dynamic languages such as R and Python.\nSome of the reasons FUGE_LC, the original implementation, was not suitable for that:\n\nit uses (and old version of) the Qt C++ framework, which, even though Qt is now open-source, makes it quite difficult to bundle with a R or Python package. And it was quite difficult to setup and build.\nit can only be used via a javascript script interpreted internally, that makes it really difficult to use properly from another language, especially to control the iterations of the algorithm.\nwe wanted to add new features.\n\nThe characteristics of this re-implementation are:\n\neverything had to be rewritten, since the existing code made heavy use of Qt data structure and base classes, and was not designed for unit-testing. But all the algorithms and calculations are the same.\nit uses standard C++17 and its standard library with not a single external dependency, making it easy to bundle with for instance a R package.\nit includes 2 related new features, prototyped by Magali Egger: features importance and genetic population biased initialization, based on those features importance. That should also be a future post.\nit is available as a C++ shared and static library, but still provides a C++ executable, as FUGE_LC.\nit is extremely well tested: 100% test code coverage (). It is commonly assumed that a test coverage over 90% or 95% is overkill. For sure the last percents are by far the hardest to fix, but I am such a TDD (Test Driven Development) fanatic that I did. I always learn some new insights in programming and code design in that exercise. I’ll probably write a post about unit testing and the related tools if there is some interest.\nthe test-driven design makes it really modular, so that new features can be added more easily.\nsince the goal is to publish a R package, the code has to be portable, at least on the 3 main operating systems that the CRAN supports: Linux, MacOs and Windows. With C++ it’s really difficult, since each OS and compiler has its peculiarities. Using the github CI (Continuous Integration), named github actions, the code is automatically tested on all 3 platforms.\nthe software is of course reproducible, meaning that with the same input (including the random seed), we get the same output. Actually I spotted that it was not true for FUGE_LC, and Magali Egger fixed that.\nit is also cross-platform reproducible. I mean the same input (including the random seed) will get the very same output on all 3 supported plaforms, and I actually had a hard-time achieving that. I’ll also probably write a post about that.\nthe current code has not been optimized for speed (yet), but for correctness and compatibility. But some obvious inefficiencies have been fixed. There is for sure plenty of room for optimization.\n\nI am currently working on the R package called Rfuzzycoco. It is already working but I am preparing for the CRAN submission.\nLet me know if you are interested by this project."
  },
  {
    "objectID": "posts/organizing_dev_with_srcpkgs/index.html",
    "href": "posts/organizing_dev_with_srcpkgs/index.html",
    "title": "Organizing R development using srcpkgs",
    "section": "",
    "text": "This is an introduction on organizing R projects using source packages (powered by my R package srcpkgs). It is based on notes for a talk I have on 2024-05-27 for the Swiss Institute of Bioinformatics Vital-IT group Analysts meeting.\nThe objective is to organize R projects in order to:\n\nreuse code\nshare code\nincrease robustness\nenable analysis (code) reproducibility\n\nThe context is mostly for analysis oriented R projects.\n\n\nAll R users use R packages, the core ones such as base, stats, tools, and some from CRAN or BioConductor.\nWhy would you want to use R packages for your own code???\na R package is:\n\nself-contained\n\nit bundles together all related code, the documentation, the relevant data and tests\n\nthe dependencies are explicitly stated, and are themselves R packages"
  },
  {
    "objectID": "posts/organizing_dev_with_srcpkgs/index.html#overview",
    "href": "posts/organizing_dev_with_srcpkgs/index.html#overview",
    "title": "Organizing R development using srcpkgs",
    "section": "",
    "text": "This is an introduction on organizing R projects using source packages (powered by my R package srcpkgs). It is based on notes for a talk I have on 2024-05-27 for the Swiss Institute of Bioinformatics Vital-IT group Analysts meeting.\nThe objective is to organize R projects in order to:\n\nreuse code\nshare code\nincrease robustness\nenable analysis (code) reproducibility\n\nThe context is mostly for analysis oriented R projects.\n\n\nAll R users use R packages, the core ones such as base, stats, tools, and some from CRAN or BioConductor.\nWhy would you want to use R packages for your own code???\na R package is:\n\nself-contained\n\nit bundles together all related code, the documentation, the relevant data and tests\n\nthe dependencies are explicitly stated, and are themselves R packages"
  },
  {
    "objectID": "posts/organizing_dev_with_srcpkgs/index.html#on-the-natural-evolution-of-code-projects",
    "href": "posts/organizing_dev_with_srcpkgs/index.html#on-the-natural-evolution-of-code-projects",
    "title": "Organizing R development using srcpkgs",
    "section": "On the natural evolution of code projects…",
    "text": "On the natural evolution of code projects…\nMy view on the general evolution of analysis projects:\n\nyou start with a single script, sequential, with no functions\nat one point (after writing hundreds or thousands of lines) you realize that you need some functions\nthen you start reusing those functions across projects by copy/paste. This raises a number of problems\n\nversioning: at one point you will fix or improve such a function\n\nit may be difficult to remember which project contains the latest version\nwhat of the projects that contain the incorrect versions?\n\n\nthen you may want, if you work in a team, to share this code with colleagues, or to use theirs\n\n–&gt; it requires some documentation, even terse.\nthere’s a increased responsibility. What if your code is wrong and impact the projects of your colleagues? One remedy is to write tests for those functions.\nthose functions are seldom independent, so that you can not just pick one\nall those functions are exposed (i.e public or exported).\n\nif you start to use a low-level function in your project, and that in the next version it has been refactored and that this function has been changed, or removed, updating the shared code will break the project.\n\n\nfor all those reasons you start packaging your reusable code as a R package\n\nyou can add documentation, tests, group code logically. It brings a namespace so that you can decide what you expose.\n\nBut… it does NOT really solve the versioning problem\n\nin R, packages have to be installed (e.g. using install.packages()) before you can use them with library(mypkg)\npackages have a version number (N.B: this is not the same as code versioning)\nif you use version v1 in your project A, and version v2 in project B, you have to juggle with versions (install/uninstall) Of course there are some tools to deal with that (renv…) but they work with external packages (or you need some private custom repositories)\nand it’s very cumbersome. Suppose that in your project A you find a bug in the (installed package). In order to fix it, you need to\n\nfetch the source code of the package\ntry to reproduce your problem. Chances are that you need your project data, you have to reproduce your session\nfinally, if you manage to fix it. You have to publish it, install it.\n\n\nmy approach is to use what I call R source packages\n\nthey are normal R packages, but instead of installing them on your R system, you load them directly from source in your R session.\nit was made possible by the infamous Hadley Wickham, and his devtools::load_all() function, that mimics the loading of an installed package\nthis greatly helps with all those problems:\n\nyou embed your source packages inside your project (as git submodules, we’ll that see later) this solves the versioning/reproducibility at your reusable code level: all your projects may use a different version\n\nif you need to fix a bug, or improve and augment your reusable code, it’s a simple as editing the code for your project. And using srcpkgs, you can even easily reload the code inside your existing R sessions, without losing any computed data.\n\nso far so good. Then for ease of maintenance/modularity, you start splitting your reusable code by category, and develop several R packages, e.g. one for some misc utilities, one for loading data from your database, one for some specific analysis…\n\nthis is where srcpkgs become usefuls, since devtools was designed to manage a single R source package, not a collection/library of possibly inter-dependent packages.\n\nadditionally has a useful little hack that enables you to use the standard library() function to load your source packages. So that when you analysis is finalized, or deployed in production, with your packages installed in the standard way, your script will continue to worl without any change.\n\n\nBut this does not solve the reproducibility for the external packages\n\nyour code and source library most certainly use external packages, and also depend on your R version (and thus on the bioconductor version)\nit may also depend on your OS architecture (CPU…)\nthis is out of scope for that talk, but one solution for that is to use a virtualized development environment: a docker container (cf https://rocker-project.org/) that contains a fixed version of R, and of all the needed external packages.\nnow the challenge is to synchronize that docker container version with your source library version…\nalso cf devcontainers\n\n\n\nSummary\nscript --&gt; script+functions --&gt; script + source files --&gt;  R package --&gt; R source package --&gt; R source library [ + R docker env]"
  },
  {
    "objectID": "posts/organizing_dev_with_srcpkgs/index.html#my-recommended-project-setup",
    "href": "posts/organizing_dev_with_srcpkgs/index.html#my-recommended-project-setup",
    "title": "Organizing R development using srcpkgs",
    "section": "My recommended project setup",
    "text": "My recommended project setup\n\nthe source library of R packages\n\nshould be a single dedicated git repository\n\nrecommended since it’s easier to have consistent versions of interdependent packages\nbut each package could be in its own git repository if needed\n\neach package should contain tests (very important, even if it’s counter intuitive, but there is usually more value in the test suite than in the code itself, don’t get me started on that…)\nfor internal packages, especially for a public of developers I personally that the documentation is less important, for example that for a publicly released package.\nyou should use CI (Continuous Integration, like github actions or gitlab CI) to automatically run the automated tests each time you push to the repository.\nalso, reporting the test coverage is important\n\nthe project code\n\nMUST be versioned in a git repository (in github/gitlab…): one repository per project\nshould itself be a R (source) package\n\neasier to add tests, documentation, vignettes\n\nbut can be a single script or a set of source files\ncontain a given version (commit/tag/branch) of the source library as a git submodule\nshould contain a vscode devcontainer to execute the project’s code (automatically usable via github codespaces)\n\nthe project R code will then use the srcpkgs package, that will automatically discover the R packages contained in the project folder, and transparently load them using the hacked library() function as if they were installed packages."
  },
  {
    "objectID": "posts/docker_userid_fixer_intro/index.html",
    "href": "posts/docker_userid_fixer_intro/index.html",
    "title": "an elegant way to fix user IDs in docker containers using docker_userid_fixer",
    "section": "",
    "text": "It’s about a rather technical issue in using docker containers that interact with the docker host computer, generally related to using the host filesystem inside the container. That happens in particular in reproducible research context. I developed an opensource utility that helps tackling that issue."
  },
  {
    "objectID": "posts/docker_userid_fixer_intro/index.html#what-is-it-about",
    "href": "posts/docker_userid_fixer_intro/index.html#what-is-it-about",
    "title": "an elegant way to fix user IDs in docker containers using docker_userid_fixer",
    "section": "",
    "text": "It’s about a rather technical issue in using docker containers that interact with the docker host computer, generally related to using the host filesystem inside the container. That happens in particular in reproducible research context. I developed an opensource utility that helps tackling that issue."
  },
  {
    "objectID": "posts/docker_userid_fixer_intro/index.html#docker-containers-as-execution-environments",
    "href": "posts/docker_userid_fixer_intro/index.html#docker-containers-as-execution-environments",
    "title": "an elegant way to fix user IDs in docker containers using docker_userid_fixer",
    "section": "docker containers as execution environments",
    "text": "docker containers as execution environments\nThe initial and main use case of a docker container: a self-contained application that only interacts with the host system with some network ports. Think of a web application: the docker container typically contains a web server and a web application, running for example on port 80 (inside the container). The container is then run on the host, by binding the container internal port 80 to a host port (e.g. 8000). Then the only interaction between the containerized app and the host system is via this bound network port.\nContainers as execution environments are completely different:\n\ninstead of containerizing an application, it’s the application build system that is containerized.\n\nit could a be a compiler, an IDE, a notebook engine, a Quarto publishing system…\n\nthe goals are:\n\nto have an standard, easy to install and share environment\n\nimagine a complex build environment, with fixed versions of R, python and zillions of external packages. Installing everything with the right versions can be a very difficult and time-consuming task. By sharing a docker image containing everything already installed and pre-configured is a real time-saver.\n\nto have a reproducible environment\n\nby using it, you are able to reproduce some analysis results, since you are using very same controlled environment\nyou can also easily reproduce bugs, which is the first step to fixing them\n\n\n\nBut, in order to use those execution environments, those containers must have access to the host system, in particular to the host user filesystem."
  },
  {
    "objectID": "posts/docker_userid_fixer_intro/index.html#docker-containers-and-the-host-filesystem",
    "href": "posts/docker_userid_fixer_intro/index.html#docker-containers-and-the-host-filesystem",
    "title": "an elegant way to fix user IDs in docker containers using docker_userid_fixer",
    "section": "docker containers and the host filesystem",
    "text": "docker containers and the host filesystem\nSuppose you have containerized an IDE, e.g. Rstudio. Your Rstudio is installed and running inside the docker container, but it needs to read and edit files in your project folder.\nFor that you bind mount your project folder (in your host filesystem) using the docker run --volume option. Then your files are accessible from withing the docker container.\nThe challenge now are the file permissions. Suppose your host user has userid 1001, and suppose that the user owning the Rsudio process in the container is either 0 (root), or 1002.\nIf the container user is root, then it will have no issue in reading your files. But as soon as you edit some existing files, are produce new ones (e.g. pdf, html), these files will belong to root also on the host filesystem! Meaning that your local host user will not be able to use them, or delete them, since they belong to root.\nNow if the container user id is 1002, Rstudio may not be able to read your files, edit them or produce new files. Even if it can, by settings some very permissive permissions, your local host user may not be able to use them.\nOf course one bruteforce way of solving that issue is to run with root both on the host computer and withing the docker container. This is not always possible and raise some obvious critical security concerns."
  },
  {
    "objectID": "posts/docker_userid_fixer_intro/index.html#solving-the-file-owner-issue-part-1-the-docker-run---user-option",
    "href": "posts/docker_userid_fixer_intro/index.html#solving-the-file-owner-issue-part-1-the-docker-run---user-option",
    "title": "an elegant way to fix user IDs in docker containers using docker_userid_fixer",
    "section": "solving the file owner issue part 1: the docker run --user option",
    "text": "solving the file owner issue part 1: the docker run --user option\nBecause we can not know in advance what will be the host userid (here 1001), we can not pre-configure the userid of the docker container user.\ndocker run now provides a --user option that enables to create a pseudo user with some supplied userid at runtime. For example, docker run --user 1001 ... will create a docker container running with processes belonging to a user with userid 1001.\nSo what are we still discussing this issue? Isn’t it solved?\nHere some quirks about that dynamically created user:\n\nit is a pseudo user\nit does not have a home directory (/home/xxx)\nit does not appear in /etc/passwd\nit can not be preconfigured, e.g. with a bash profile, some env vars, application defaults etc…\n\nWe can work-around these problems, but it can be tedious and frustrating. What we’d really like, is to pre-configure a docker container user, and be able to dynamically change his userid at runtime…"
  },
  {
    "objectID": "posts/docker_userid_fixer_intro/index.html#solving-the-file-owner-issue-part-2-enter-docker_userid_fixer",
    "href": "posts/docker_userid_fixer_intro/index.html#solving-the-file-owner-issue-part-2-enter-docker_userid_fixer",
    "title": "an elegant way to fix user IDs in docker containers using docker_userid_fixer",
    "section": "solving the file owner issue part 2: enter docker_userid_fixer",
    "text": "solving the file owner issue part 2: enter docker_userid_fixer\ndocker_userid_fixer is an open source utility intended to be used as a docker entrypoint to fix the userid issue I just raised.\nLet’s see how to use it: you set it as your docker ENTRYPOINT, specifying which user should be used and have his userid dynamically modified:\nENTRYPOINT [\"/usr/local/bin/docker_userid_fixer\",\"user1\"]\nLet’s be precise in our terms:\n\nthe target user, is the user requested to docker_userid_fixer, here user1\nthe requested user, is the user provisioned by docker run, i.e the user that (intially) owns the first process (PID 1)\n\nThen, at the container runtime creation, there are two options:\n\neither the requested userid (already) matches the target userid, then nothing has to be changed\nor it does not. For example the requested userid is 1001, and the target userid is 100. Then, docker_userid_fixer will fix the userid of the target user user1 from 1000 to 1001, directly in the container main process.\n\nSo in practice this solves our issue:\n\nif you do not need to fix your container userid, just use docker run the usual way (without the --user option)\nor you use --user option, then in addition of running your main process with a userid you requested, it will modify your pre-configured user to your requested userid, so that your container is running with your intended user and intended userid."
  },
  {
    "objectID": "posts/docker_userid_fixer_intro/index.html#docker_userid_fixer-setup",
    "href": "posts/docker_userid_fixer_intro/index.html#docker_userid_fixer-setup",
    "title": "an elegant way to fix user IDs in docker containers using docker_userid_fixer",
    "section": "docker_userid_fixer setup",
    "text": "docker_userid_fixer setup\nYou can find instructions about the setup here.\nBut it boils down to:\n\nbuild or download the tiny executable (17k)\ncopy it into your docker image\nmake it executable as setuid root\nconfigure it as your entrypoint"
  },
  {
    "objectID": "posts/docker_userid_fixer_intro/index.html#the-gory-details",
    "href": "posts/docker_userid_fixer_intro/index.html#the-gory-details",
    "title": "an elegant way to fix user IDs in docker containers using docker_userid_fixer",
    "section": "the gory details",
    "text": "the gory details\nI have put some short notes https://github.com/kforner/docker_userid_fixer#how-it-works but I’ll try to rephrase.\nThe crux of the implementation is the setuid root of the docker_userid_fixer executable in the container. We need root permissions to change the userid, and this setuid enables that privileged execution only for the docker_userid_fixerprogram, and that for a very short time.\nAs soon as the userid has been modified if needed, docker_userid_fixer will switch the main process to the requested user (and userid!)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a computer scientist (and mathematician) by training, and have always worked in bioinformatics and pharmaceutical field. I am really experienced in R programming (but I also coded in Perl, Python, C++, java) and more generally in setting up and leading software development platforms, in particular for reproducible research. In particular I pioneered the use of docker containers as reproducible computing development environments.\nI am also fond of software optimization and parallelization.\nLately, I’m working as a consultant, so don’t hesitate to contact me if you like my work.\nAnd I am a fervent proponent of OpenSource software: my OpenSource contributions"
  },
  {
    "objectID": "posts/preparing_rfuzzycoco_for_cran/index.html",
    "href": "posts/preparing_rfuzzycoco_for_cran/index.html",
    "title": "Preparing Rfuzzycoco for publication on CRAN",
    "section": "",
    "text": "I recently released on github a R package Rfuzzycoco that provides the Fuzzy Coco algorithm by wrapping my fuzzycoco C++ implementation and extending it. It provides easy installation and access to this software.\nThe Comprehensive R Archive Network (CRAN) is R’s main package repository. The quality of CRAN packages is enforced by a very drastic process of submission, that covers the code itself, the dependencies, the size of the package, the portability of file encoding and filenames, the documentation, the description of the package, the code examples etc…\nHaving a package accepted can be a daunting and very time-consuming task, so that some developers just give up and release their package by other means, for example on github.\nIt is even much worse for packages with C++ code, because the package has to implement the build process in a portable way, and the package should work on the 3 major platforms: Linux, MacOs and Windows, that use different compilers and implementations of the C++ standard library.\nOn the other hand, having his package on CRAN is a guarantee of quality and portability. There are also some useful services for the users, as the distribution of binary packages, or Debian/ubuntu APT packages. For developers, when you submit a new version there are automated checks against all reverse dependencies, i.e. all packages using your package, for regression testing.\nI will briefly explain how I am preparing for submitting Rfuzzycoco to the CRAN, the ecosystem and tools that I use. Some are very common and straightforward.\n\nI use the wonderful devtools package to develop and test the package code.\ndocumentation:\n\nreference manual: I use roxygen2 to generate the function-level documentation from inline annotations in the source code. It is integrated in devtools.\nvignettes: I use rmarkdown.\nwebsite: I use pkgdown to generate the HTML documentation from the roxygen doc and Rmarkdown vignettes and publish it on github pages via the CI\n\nunit testing:\n\nthis is in my opinion the most fundamental aspect of development, assessing the quality of code and enabling the refactoring.\nvery surprisingly, tests are not mandatory for CRAN, but they are for me.\nI use the testthat package, also integrated in devtools\nmeasuring the test coverage is also of paramount importance. I use covr for that, it is able to also cover the C/C++ code included in the R package.\nI use the codecov service to publish the test coverage results.\n: I just achieved 100% test coverage , for both the R and C++ Rfuzzycoco code (excluding the fuzzycoco lib code which by the way has also 100% test coverage). I explained in a precedent post that in general it’s not worth trying to reach 100%, but it is for me.\n\nR CMD check:\n\nthis is a fundamental tool that implements lots of checks on your package, and also run the tests in a realistic way. You should use it from the beginning. I integrate it in my Makefile (make check) and automate it in the CI.\n\ngit: of course your code must be versioned, and should use branches for developing new features.\ngithub (or equivalent devops platform). I will only discuss github here since that’s what I’m using for Rfuzzycoco\n\nit solves the distribution, the collaboration via forks and pull requests\nit provides issues for reporting bugs, and interacting with other developers and users\nit also provides documentation, via the README.md and github pages\nContinuous Integration (CI) via github actions\n\nthis also a fundamental feature. It can automate the checks, the documentation publishing and much more.\nIt can check your package on multiple platforms\nI currently have a CI for checking (R CMD check) on ubuntu, macos and windows, and on several versions of R (release and devel). This is an absolute killer feature, especially for CRAN since it can test the portability of your package.\nI also have a CI that measures the test coverage, and automatically publish it on codecov\nand a CI to publish the HTML documentation on github pages\n\n\n\nThe sooner this ecosystem is setup, the better. It for sure involves some work, but you can reuse all this infrastructure for other packages.\nAnd I think one thing that is lacking is a standard R package project that would implement all this kind of tooling in a standardized, optimized and well maintained way. That would lower the barrier to entry to R package development and would dramatically increase the overall quality.\nStay tuned for more on the Rfuzzycoco CRAN journey.\nI (Karl Forner) am currently working as a consultant, contact me if you want me to help you on using R, organizing development, developing R packages or more generally supporting your software development efforts."
  },
  {
    "objectID": "posts/preparing_rfuzzycoco_for_cran/index.html#overview",
    "href": "posts/preparing_rfuzzycoco_for_cran/index.html#overview",
    "title": "Preparing Rfuzzycoco for publication on CRAN",
    "section": "Overview",
    "text": "Overview\nThis is an introduction on organizing R projects using source packages (powered by my R package srcpkgs). It is based on notes for a talk I have on 2024-05-27 for the Swiss Institute of Bioinformatics Vital-IT group Analysts meeting.\nThe objective is to organize R projects in order to:\n\nreuse code\nshare code\nincrease robustness\nenable analysis (code) reproducibility\n\nThe context is mostly for analysis oriented R projects.\nI (Karl Forner) am currently working as a consultant, contact me if you want me to help you on using R, organizing development, developing R packages or more generally support your software development efforts."
  },
  {
    "objectID": "posts/preparing_rfuzzycoco_for_cran/index.html#on-the-natural-evolution-of-code-projects",
    "href": "posts/preparing_rfuzzycoco_for_cran/index.html#on-the-natural-evolution-of-code-projects",
    "title": "Preparing Rfuzzycoco for publication on CRAN",
    "section": "On the natural evolution of code projects…",
    "text": "On the natural evolution of code projects…\nMy view on the general evolution of analysis projects:\n\nyou start with a single script, sequential, with no functions\nat one point (after writing hundreds or thousands of lines) you realize that you need some functions\nthen you start reusing those functions across projects by copy/paste. This raises a number of problems\n\nversioning: at one point you will fix or improve such a function\n\nit may be difficult to remember which project contains the latest version\nwhat of the projects that contain the incorrect versions?\n\n\n\nI (Karl Forner) am currently working as a consultant, contact me if you want me to help you on using R, organizing development, developing R packages or more generally support your software development efforts."
  },
  {
    "objectID": "posts/preparing_rfuzzycoco_for_cran/index.html#my-recommended-project-setup",
    "href": "posts/preparing_rfuzzycoco_for_cran/index.html#my-recommended-project-setup",
    "title": "Preparing Rfuzzycoco for publication on CRAN",
    "section": "My recommended project setup",
    "text": "My recommended project setup\n\nthe source library of R packages\n\nshould be a single dedicated git repository\n\nrecommended since it’s easier to have consistent versions of interdependent packages\nbut each package could be in its own git repository if needed\n\neach package should contain tests (very important, even if it’s counter intuitive, but there is usually more value in the test suite than in the code itself, don’t get me started on that…)\nfor internal packages, especially for a public of developers I personally that the documentation is less important, for example that for a publicly released package.\nyou should use CI (Continuous Integration, like github actions or gitlab CI) to automatically run the automated tests each time you push to the repository.\nalso, reporting the test coverage is important\n\n\nI (Karl Forner) am currently working as a consultant, contact me if you want me to help you on using R, organizing development, developing R packages or more generally support your software development efforts."
  }
]